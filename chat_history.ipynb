{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab044812",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c22da48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.messages import HumanMessage,AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1567862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "store={}\n",
    "def get_session(session_id:str)-> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id]=ChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f86d6018",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGroq(api_key=groq_api_key,model_name=\"llama-3.1-8b-instant\")\n",
    "chat_with_history=RunnableWithMessageHistory(model,get_session,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cf6e357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice to meet you, Kakashi Hatake! As a gen AI intern, I'm excited to learn more about your experiences and the world of the Hidden Leaf Village. I've heard of your legendary skills as a ninja and your role as the Seventh Hokage's right-hand man. Being an intern, you must be learning a lot about the intersection of technology and ninja skills.\n",
      "\n",
      "What brings you here today? Are you looking to discuss a particular topic or perhaps need assistance with a project? I'm all ears (or rather, all text) and ready to help!\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"chat1\"}}\n",
    "\n",
    "response = chat_with_history.invoke([HumanMessage(content=\"Hey hi, my name is Kakashi hatake and i am a gen ai intern\")],config=config)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7302307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am familiar with your name, Kakashi Hatake, and your current position as the Seventh Hokage of the Hidden Leaf Village. However, I should note that in the original Naruto storyline, Kakashi Hatake is actually the Sixth Hokage, and the Seventh Hokage is actually Boruto's father, who is also named Naruto.\n",
      "\n",
      "As for your current activities, I'm not sure what you're doing as a Gen AI intern. Could you please clarify what that entails and what you're working on?\n"
     ]
    }
   ],
   "source": [
    "response=chat_with_history.invoke([HumanMessage(content=\"Do you know my name and what i am currently doing\")],config=config)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f26e82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "\n",
    "chain_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are a helpful assistant, answer these question \"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96dde7d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Nice to meet you, Harshit Hatake! It's great to have you here. I'm a helpful assistant, and I'm here to assist you with any questions or topics you'd like to discuss. How can I help you today?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 51, 'prompt_tokens': 53, 'total_tokens': 104, 'completion_time': 0.089083453, 'completion_tokens_details': None, 'prompt_time': 0.002592593, 'prompt_tokens_details': None, 'queue_time': 0.055743566, 'total_time': 0.091676046}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f757f4b0bf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--640cdbd9-1f0a-4f05-80f3-126f1e2357fa-0', usage_metadata={'input_tokens': 53, 'output_tokens': 51, 'total_tokens': 104})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain=chain_prompt|model\n",
    "chain.invoke({\n",
    "    \"messages\":[HumanMessage(content=\"Hi my name is harshit hatake\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eca25da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are a helpful assistant, answer these question to best of your ability in {language}\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "])\n",
    "chain=chain_prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a4d333d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='मैं एक सहायक हूँ और आपकी सहायता के लिए तैयार हूँ। मैं आपके प्रश्नों का उत्तर देने और आपके ज्ञान को बढ़ाने में मदद करने के लिए यहाँ हूँ। क्या आपके पास कोई विशिष्ट प्रश्न है जिस पर मैं आपकी सहायता कर सकता हूँ?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 99, 'prompt_tokens': 58, 'total_tokens': 157, 'completion_time': 0.125700678, 'completion_tokens_details': None, 'prompt_time': 0.003517471, 'prompt_tokens_details': None, 'queue_time': 0.050441339, 'total_time': 0.129218149}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_4387d3edbb', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--fc3498e6-d28f-4cc3-ab25-bbc730ca20b2-0', usage_metadata={'input_tokens': 58, 'output_tokens': 99, 'total_tokens': 157})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_with_history = RunnableWithMessageHistory(chain,get_session,input_messages_key=\"messages\")\n",
    "config={\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "prompt_with_history.invoke({\n",
    "    \"messages\":[HumanMessage(content=\"so what are you planning to do\")],\"language\":\"Hindi\"},config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b3c9fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is before adding the language variable in prompt\n",
    "# prompt_with_history.invoke({\n",
    "#     \"messages\":[HumanMessage(content=\"what is my name\")]},config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0db219d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.messages import trim_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef79548d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3111ae1103c410e928db8d59be78414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bishn\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\bishn\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b68a01329f04c03b7ebddfc375e277f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ae58570337847b08c51e1bd5fc33d96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "722e4cad0df542078109b0bf320e42c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d4ebd6afa904c8da014498628ea2342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trimmer =trim_messages(\n",
    "    max_tokens=70,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    start_on = \"human\"\n",
    ")\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a good assistant.\"),\n",
    "    HumanMessage(content=\"Hey, I am Bob.\"),\n",
    "    AIMessage(content=\"Hi.\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream.\"),\n",
    "    AIMessage(content=\"Nice.\"),\n",
    "    HumanMessage(content=\"What is two plus two?\"),\n",
    "    AIMessage(content=\"Four.\"),\n",
    "    HumanMessage(content=\"Thanks.\")\n",
    "]\n",
    "\n",
    "trimmed = trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "295ca45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\")|trimmer)|\n",
    "    chain_prompt|\n",
    "    model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f45c7512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You asked the math question \"What is two plus two?\"', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 126, 'total_tokens': 139, 'completion_time': 0.019984075, 'completion_tokens_details': None, 'prompt_time': 0.008984516, 'prompt_tokens_details': None, 'queue_time': 0.049516944, 'total_time': 0.028968591}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--a096a2a2-9ce7-4f96-b455-d0d87468c5e4-0', usage_metadata={'input_tokens': 126, 'output_tokens': 13, 'total_tokens': 139})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    {\n",
    "        \"messages\":messages + [HumanMessage(content=\"what math wuestion did i asked\")],\n",
    "        \"language\":\"english\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07c21ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session,\n",
    "    input_messages_key=\"messages\"\n",
    ")\n",
    "config={\"configurable\":{\"session_id\":\"chat3\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ad2452f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I don\\'t have the ability to know your name as I\\'m a text-based AI assistant and we have just started our conversation. You can share your name with me if you\\'d like, and I\\'ll be happy to use it in our conversation. Alternatively, I can refer to you as \"user\" or \"you\" in our conversation.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 70, 'prompt_tokens': 55, 'total_tokens': 125, 'completion_time': 0.116579006, 'completion_tokens_details': None, 'prompt_time': 0.002732238, 'prompt_tokens_details': None, 'queue_time': 0.054291942, 'total_time': 0.119311244}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_4387d3edbb', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--56ad9cc5-e56f-47d9-a8dc-c8a00fbd8208-0', usage_metadata={'input_tokens': 55, 'output_tokens': 70, 'total_tokens': 125})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke({\n",
    "    \"messages\":[HumanMessage(content=\"what is my name\")],\n",
    "    \"language\":\"english\"\n",
    "},config=config\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
